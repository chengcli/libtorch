# How to build libtorch locally
CMAKE_INSTALL_DIR=XX python setup.py develop

This steps up an egg.link to torch directory, `torch.egg-link`
```
/Users/chengcli/Development/libtorch
.
```

python setup.py install


# Example cmake configuration
-- ******** Summary ********
-- General:
--   CMake version         : 3.29.6
--   CMake command         : /opt/homebrew/Cellar/cmake/3.29.6/bin/cmake
--   System                : Darwin
--   C++ compiler          : /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/c++
--   C++ compiler id       : AppleClang
--   C++ compiler version  : 15.0.0.15000309
--   Using ccache if found : ON
--   Found ccache          : CCACHE_PROGRAM-NOTFOUND
--   CXX flags             :  -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DTMP_USE_TSC_AS_TIMESTAMP -DLIBKINETO_NOCUPTI -DLIBKINETO_NOROCTRACER -DUSE_PYTORCH_QNNPACK -DAT_BUILD_ARM_VEC256_WITH_SLEEF -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=braced-scalar-init -Werror=range-loop-construct -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wvla-extension -Wsuggest-override -Wnewline-eof -Winconsistent-missing-override -Winconsistent-missing-destructor-override -Wno-pass-failed -Wno-error=pedantic -Wno-error=old-style-cast -Wno-error=inconsistent-missing-override -Wno-error=inconsistent-missing-destructor-override -Wconstant-conversion -Wno-invalid-partial-specialization -Wno-missing-braces -Qunused-arguments -fcolor-diagnostics -faligned-new -Wno-unused-but-set-variable -fno-math-errno -fno-trapping-math -Werror=format -DUSE_MPS -Wno-unused-private-field -Wno-missing-braces
--   Shared LD flags       :  -rdynamic -weak_framework Foundation -weak_framework MetalPerformanceShaders -weak_framework MetalPerformanceShadersGraph -weak_framework Metal
--   Static LD flags       : 
--   Module LD flags       : 
--   Build type            : Release
--   Compile definitions   : ONNX_ML=1;ONNXIFI_ENABLE_EXT=1;ONNX_NAMESPACE=onnx_torch;HAVE_MMAP=1;_FILE_OFFSET_BITS=64;HAVE_SHM_OPEN=1;HAVE_SHM_UNLINK=1;USE_EXTERNAL_MZCRC;MINIZ_DISABLE_ZIP_READER_CRC32_CHECKS;FLASHATTENTION_DISABLE_ALIBI;AT_BUILD_ARM_VEC256_WITH_SLEEF
--   CMAKE_PREFIX_PATH     : /Users/chengcli/torch/lib/python3.12/site-packages
--   CMAKE_INSTALL_PREFIX  : /Users/chengcli/Development/libtorch/torch
--   USE_GOLD_LINKER       : OFF
-- 
--   TORCH_VERSION         : 2.5.0
--   BUILD_STATIC_RUNTIME_BENCHMARK: OFF
--   BUILD_BINARY          : OFF
--   BUILD_CUSTOM_PROTOBUF : ON
--     Link local protobuf : ON
--   BUILD_DOCS            : OFF
--   BUILD_PYTHON          : True
--     Python version      : 3.12.3
--     Python executable   : /Users/chengcli/torch/bin/python
--     Python library      : /opt/homebrew/opt/python@3.12/Frameworks/Python.framework/Versions/3.12/lib/libpython3.12.dylib
--     Python includes     : /opt/homebrew/opt/python@3.12/Frameworks/Python.framework/Versions/3.12/include/python3.12
--     Python site-package : /Users/chengcli/torch/lib/python3.12/site-packages
--   BUILD_SHARED_LIBS     : ON
--   CAFFE2_USE_MSVC_STATIC_RUNTIME     : OFF
--   BUILD_TEST            : True
--   BUILD_JNI             : OFF
--   BUILD_MOBILE_AUTOGRAD : OFF
--   BUILD_LITE_INTERPRETER: OFF
--   CROSS_COMPILING_MACOSX : 
--   INTERN_BUILD_MOBILE   : 
--   TRACING_BASED         : OFF
--   USE_BLAS              : 1
--     BLAS                : accelerate
--     BLAS_HAS_SBGEMM     : 
--   USE_LAPACK            : 1
--     LAPACK              : accelerate
--   USE_ASAN              : OFF
--   USE_TSAN              : OFF
--   USE_CPP_CODE_COVERAGE : OFF
--   USE_CUDA              : OFF
--   USE_XPU               : OFF
--   USE_ROCM              : OFF
--   BUILD_NVFUSER         : 
--   USE_EIGEN_FOR_BLAS    : ON
--   USE_FBGEMM            : OFF
--     USE_FAKELOWP          : OFF
--   USE_KINETO            : ON
--   USE_GFLAGS            : OFF
--   USE_GLOG              : OFF
--   USE_LITE_PROTO        : OFF
--   USE_PYTORCH_METAL     : OFF
--   USE_PYTORCH_METAL_EXPORT     : OFF
--   USE_MPS               : ON
--   USE_MKL               : OFF
--   USE_MKLDNN            : OFF
--   USE_UCC               : OFF
--   USE_ITT               : OFF
--   USE_NCCL              : OFF
--   USE_NNPACK            : ON
--   USE_NUMPY             : OFF
--   USE_OBSERVERS         : ON
--   USE_OPENCL            : OFF
--   USE_OPENMP            : OFF
--   USE_MIMALLOC          : OFF
--   USE_VULKAN            : OFF
--   USE_PROF              : OFF
--   USE_PYTORCH_QNNPACK   : ON
--   USE_XNNPACK           : ON
--   USE_DISTRIBUTED       : OFF
--   Public Dependencies  : 
--   Private Dependencies : Threads::Threads;pthreadpool;cpuinfo;pytorch_qnnpack;nnpack;XNNPACK;fp16;foxi_loader;fmt::fmt-header-only;kineto
--   Public CUDA Deps.    : 
--   Private CUDA Deps.   : 
--   USE_COREML_DELEGATE     : OFF
--   BUILD_LAZY_TS_BACKEND   : ON
--   USE_ROCM_KERNEL_ASSERT : OFF
-- Configuring done (29.0s)
-- Generating done (1.6s)
-- Build files have been written to: /Users/chengcli/Development/libtorch/build

## How to use tensor
https://stackoverflow.com/questions/59676983/in-torch-c-api-how-to-write-to-the-internal-data-of-a-tensor-fastly

This version is very slow:
```cpp
    torch::Tensor tensor = torch::empty({1000, 1000});
    for(int i=0; i < 1000; i++)
    {
        for(int j=0 ; j < 1000; j++)
        {
            tensor[i][j] = calc_tensor_data(i,j);
        }
    }
```

Use the following version to properly access the point-wise tensor data:
```cpp
    torch::Tensor tensor = torch::empty({1000, 1000});
    auto accessor = tensor.accessor<float,2>();
    for(int i=0; i < 1000; i++)
    {
        for(int j=0 ; j < 1000; j++)
        {
            accessor[i][j] = calc_tensor_data(i,j);
        }
    }
```

## How to use your operator
https://pytorch.org/tutorials/advanced/torch_script_custom_ops.html

```python
import torch
x = torch.tensor([1.0, 2.0, 3.0])
y = torch.ops.Euler.cons2prim(x)

## How to write a kernel, an example of special function

Here we use the example of special function, the entropy function,
to show how to write a custom kernel. The entropy function is defined as:

```math
H(x) = -x * log(x)
```

For a positive value of x. If x is zero, we define H(x) = 0.
If x is negative, we define H(x) = -inf.
This function is evaluated element-wise on any input tensor.

To begin with, add the following two entries to the `native_functions.yaml` file:

```yaml
- func: special_entr(Tensor self) -> Tensor
  structured_delegate: special_entr.out
  python_module: special
  variants: function
  tags: pointwise

- func: special_entr.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
  structured: True
  structured_inherits: TensorIteratorBase
  python_module: special
  variants: function
  dispatch:
    CPU, CUDA: special_entr_out
  tags: pointwise
```

This declares the `special_entr` function, which takes a single input tensor and returns a single output tensor.
The second entry declares the `special_entr.out` function, which takes an input tensor and an output tensor, and writes the result to the output tensor.
The first function has immutable input, while the second function is mutable.

Using the `structured_delegate` and `structured_inherits` fields, we can specify that the second function is a structured delegate of the first function.
This means that the second function is a structured function that is called by the first function.

Using the `torchgen/gen.py` script, we can generate header files for the functions:

build/aten/src/ATen/ops
special_entr.h
special_entr_native.h
special_entr_ops.h
special_entr_cpu_dispatch.h
special_entr_cuda_dispatch.h

Particular, the `special_entr_ops.h` file declares the structure delegate in namespace at::_ops
```cpp file
namespace at {
namespace _ops {

struct TORCH_API special_entr {
  using schema = at::Tensor (const at::Tensor &);
  using ptr_schema = schema*;
  // See Note [static constexpr char* members for windows NVCC]
  STATIC_CONSTEXPR_STR_INL_EXCEPT_WIN_CUDA(name, "aten::special_entr")
  STATIC_CONSTEXPR_STR_INL_EXCEPT_WIN_CUDA(overload_name, "")
  STATIC_CONSTEXPR_STR_INL_EXCEPT_WIN_CUDA(schema_str, "special_entr(Tensor self) -> Tensor")
  static at::Tensor call(const at::Tensor & self);
  static at::Tensor redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self);
};

struct TORCH_API special_entr_out {
  using schema = at::Tensor & (const at::Tensor &, at::Tensor &);
  using ptr_schema = schema*;
  // See Note [static constexpr char* members for windows NVCC]
  STATIC_CONSTEXPR_STR_INL_EXCEPT_WIN_CUDA(name, "aten::special_entr")
  STATIC_CONSTEXPR_STR_INL_EXCEPT_WIN_CUDA(overload_name, "out")
  STATIC_CONSTEXPR_STR_INL_EXCEPT_WIN_CUDA(schema_str, "special_entr.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")
  static at::Tensor & call(const at::Tensor & self, at::Tensor & out);
  static at::Tensor & redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out);
};

} // namespace _ops
} // namespace at
```

The `special_entr.h` file declares the API function in namespace at
```cpp file
namespace at {

// aten::special_entr(Tensor self) -> Tensor
inline at::Tensor special_entr(const at::Tensor & self) {
    return at::_ops::special_entr::call(self);
}

// aten::special_entr.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
inline at::Tensor & special_entr_out(at::Tensor & out, const at::Tensor & self) {
    return at::_ops::special_entr_out::call(self, out);
}
// aten::special_entr.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
inline at::Tensor & special_entr_outf(const at::Tensor & self, at::Tensor & out) {
    return at::_ops::special_entr_out::call(self, out);
}

} // namespace at
```

The actual dispatch functions have the same name but in different namespaces, `at::cpu` and `at::cuda`, respectively.
E.g., the `special_entr_cpu_dispatch.h` file declares the CPU dispatch function in namespace at::cpu
```cpp file
namespace at {
namespace cpu {

TORCH_API at::Tensor special_entr(const at::Tensor & self);
TORCH_API at::Tensor & special_entr_out(at::Tensor & out, const at::Tensor & self);
TORCH_API at::Tensor & special_entr_outf(const at::Tensor & self, at::Tensor & out);

} // namespace cpu
} // namespace at
```

The `special_entr_cuda_dispatch.h` file declares the CUDA dispatch function in namespace at::cuda
```cpp file 
namespace at {
namespace cuda {

TORCH_API at::Tensor special_entr(const at::Tensor & self);
TORCH_API at::Tensor & special_entr_out(at::Tensor & out, const at::Tensor & self);
TORCH_API at::Tensor & special_entr_outf(const at::Tensor & self, at::Tensor & out);

} // namespace cuda
} // namespace at
```


Additionally, the `RegisterCPU.cpp` registers the structuring function for the CPU backend:

```cpp
struct structured_special_entr_out_functional final : public at::native::structured_special_entr_out {
  ...
  std::array<Tensor, 1> outputs_;
}

struct structured_special_entr_out_out final : public at::native::structured_special_entr_out {
  ...
  std::array<std::reference_wrapper<Tensor>, 1> outputs_;
  std::array<::std::optional<Tensor>, 1> proxy_outputs_;
}
```

And wrapper functions:
```cpp
at::Tensor wrapper_CPU_special_entr(const at::Tensor & self) {
  structured_special_entr_out_functional op;
  op.meta(self);
  op.impl(self, op.outputs_[0]);
  return std::move(op.outputs_[0]);
}

at::Tensor & wrapper_CPU_special_entr_out_out(const at::Tensor & self, at::Tensor & out) {
  structured_special_entr_out_out op(out);
  op.meta(self);
  op.impl(self, op.maybe_get_output(0));
  if (op.proxy_outputs_[0].has_value()) op.outputs_[0].get().copy_(*op.proxy_outputs_[0]);
  return out;
}
```

In `UnaryOps.cpp`, the actual implementation of the function is defined:
```cpp
CREATE_UNARY_FLOAT_META_FUNC(special_entr)
CREATE_UNARY_TORCH_IMPL_FUNC(special_entr_out, special_entr_stub)
DEFINE_DISPATCH(special_entr_stub);
```

The actual function that does the calculation is the `special_entr_stub` function, which is defined in `cpu/UnaryOpsKernel.cpp`:
```cpp
static void entr_kernel(TensorIteratorBase& iter) {
  REGISTER_DISPATCH(special_entr_stub, &CPU_CAPABILITY::entr_kernel);
  AT_DISPATCH_FLOATING_TYPES_AND2(
      kBFloat16, kHalf, iter.common_dtype(), "entr_cpu", [&] {
        cpu_kernel(iter, [](scalar_t x) -> scalar_t {
          if (at::_isnan(x)) {
            return x;
          } else if (x > 0) {
            return -x * std::log(x);
          } else if (x == 0) {
            return static_cast<scalar_t>(0);
          }
          return static_cast<scalar_t>(-INFINITY);
        });
      });
}
```

Similarly, the CUDA implementation is defined in `cuda/UnaryOpsKernel.cu`:
```cpp
      AT_DISPATCH_COMPLEX_TYPES_AND(kComplexHalf, common_dtype, "rsqrt_cuda", [&]() {
        gpu_kernel(iter, []GPU_LAMBDA(scalar_t a) -> scalar_t {
          using opmath_t = at::opmath_type<scalar_t>;
          return rsqrt_wrapper(static_cast<opmath_t>(a));
        });
      });
```

# other links
https://support.huawei.com/enterprise/en/doc/EDOC1100191776/9f8c105f/registering-operator-development
https://docs.google.com/document/d/1YA4ZG0eTywBv-5Nlaejo9Ho-r95KJZZ4c1t37EPlJQk/edit
https://docs.google.com/presentation/d/1hvh4pAqdbbFd0WMaZRoUp_ZLue7vTzoKCCg30Xg7H-0/edit#slide=id.gc36d913cc7_2_84

# how to use python api
torch.ops.aten.cons2prim(x)
